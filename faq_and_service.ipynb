{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "from matplotlib.colors import ListedColormap\n",
    "from numpy.random import choice\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models import FastText,KeyedVectors,fasttext\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel, AutoModel\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import cluster\n",
    "\n",
    "import seaborn as sns\n",
    "import umap\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='models/sbert_large_mt_nlu_ru/4'\n",
    "model = TFAutoModel.from_pretrained(path)\n",
    "#model = AutoModel.from_pretrained(path)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    attention_mask_unsqeezed=tf.expand_dims(attention_mask,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    \n",
    "    attention_mask_unsqueezed=tf.expand_dims(attention_mask, axis=-1)\n",
    "    \n",
    "    input_mask_expanded =attention_mask_unsqueezed.expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-be049b69e73e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#Perform pooling. In this case, mean pooling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0msentence_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_pooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0msentence_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-7bf9ff792f29>\u001b[0m in \u001b[0;36mmean_pooling\u001b[1;34m(model_output, attention_mask)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmean_pooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtoken_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#First element of model_output contains all token embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0minput_mask_expanded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msum_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minput_mask_expanded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msum_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_mask_expanded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "\n",
    "#Sentences we want sentence embeddings for\n",
    "sentences = ['Привет! Как твои дела?',\n",
    "             'А правда, что 42 твое любимое число?']\n",
    "#Load AutoModel from huggingface model repository\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_mt_nlu_ru\")\n",
    "#model = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_mt_nlu_ru\")\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='tf')\n",
    "#Compute token embeddings\n",
    "\n",
    "model_output = model(**encoded_input)\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('faq.csv', delimiter=',',encoding='utf-8',dtype=str,usecols=(0,1))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine():\n",
    "    def __init__(self,faq,sbert_name=False,fasttext_path=False):\n",
    "        self.faq=faq\n",
    "        if sbert_name:\n",
    "            start = time.time()\n",
    "            \n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            self.sbert_model = AutoModel.from_pretrained(sbert_name)\n",
    "            self.sbert_model.to(self.device)\n",
    "            self.sbert_tokenizer = AutoTokenizer.from_pretrained(sbert_name)\n",
    "            self.sbert_faq_embs=np.array([self.vectorize_sentence_sbert(sent) for sent in self.faq[:,0]])\n",
    "            \n",
    "            end = time.time()\n",
    "            self.MorphAnalyzer=morph = pymorphy2.MorphAnalyzer()\n",
    "            print('Sbert model and faq successfully loaded\\nPassed time:',round(end - start,2),'s')\n",
    "            \n",
    "            \n",
    "        if fasttext_path:\n",
    "            self.ft_model= fasttext.FastTextKeyedVectors.load(fasttext_path)\n",
    "            self.ft_faq_embs=np.array([self.vectorize_sentence_ft(sent) for sent in self.faq[:,0]])\n",
    "        \n",
    "    def vectorize_sentence_ft(self,sentence):\n",
    "        txt=sentence.split()\n",
    "        val=0\n",
    "        for word in txt:\n",
    "            val+=self.ft_model[word]\n",
    "\n",
    "        return val/len(txt)\n",
    "    \n",
    "\n",
    "    def vectorize_sentence_sbert(self,sentence):\n",
    "    \n",
    "        encoded_input = self.sbert_tokenizer(sentence,\n",
    "                                             padding=True,\n",
    "                                             truncation=True,\n",
    "                                             max_length=24,\n",
    "                                             return_tensors='pt').to(self.device)\n",
    "\n",
    "   #     with torch.no_grad():\n",
    "        model_output = self.sbert_model(**encoded_input)\n",
    "            \n",
    "        #Perform pooling. In this case, mean pooling\n",
    "        sentence_embedding = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embedding = np.squeeze(sentence_embedding)\n",
    "        \n",
    "        return sentence_embedding.cpu().data.numpy()\n",
    "    \n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def mean_pooling(cls,model_output, attention_mask):\n",
    "        #Mean Pooling - Take attention mask into account for correct averaging\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def search_faq(self,question,eps,minimal_score,mode,beta):\n",
    "        if mode=='fasttext':\n",
    "            vectorizer=self.vectorize_sentence_ft\n",
    "            faq_embs=self.ft_faq_embs\n",
    "        elif mode=='sbert':\n",
    "            vectorizer=self.vectorize_sentence_sbert\n",
    "            faq_embs=self.sbert_faq_embs\n",
    "\n",
    "        question_emb=vectorizer(question)\n",
    "        score=np.zeros((self.faq.shape[0],1))\n",
    "\n",
    "       \n",
    "        for i,faq_emb in enumerate(faq_embs):\n",
    "            score[i,0]=self.cos_dist( faq_emb,question_emb)\n",
    "                 \n",
    "        \n",
    "        faq_logits=np.concatenate([faq,faq_embs,score],axis=1)\n",
    "        faq_logits=faq_logits[faq_logits[:, 3].argsort()]\n",
    "        faq_logits=faq_logits[::-1]\n",
    "\n",
    "\n",
    "        max_score=faq_logits[0,3]\n",
    "        display_num=0\n",
    "\n",
    "        for scr in faq_logits[:,2]:\n",
    "            if max_score-scr<eps and scr > minimal_score:\n",
    "                display_num+=1\n",
    "            else:\n",
    "                break\n",
    "         \n",
    "\n",
    "        \n",
    "        return faq_logits[:display_num]\n",
    "    \n",
    "    @classmethod\n",
    "    def cos_dist(cls,vect1,vect2):\n",
    "        return 1-spatial.distance.cosine(vect1,vect2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def calculate_faq(engine,question_emb,faq_embs,eps,minimal_score,mode):\n",
    "\n",
    "        score=np.zeros((engine.faq.shape[0],1))\n",
    "        for i,faq_emb in enumerate(faq_embs):\n",
    "            \n",
    "            distance=1-spatial.distance.cosine( faq_emb,question_emb)\n",
    "            score[i,0]=round(distance,4)\n",
    "                 \n",
    "        index=np.arange(len(engine.faq)).reshape((-1,1))\n",
    "        \n",
    "        questions=engine.faq[:,0].reshape((-1,1))\n",
    "        \n",
    "        faq_logits=np.concatenate([index,score],axis=1)\n",
    "        faq_logits=faq_logits[faq_logits[:, 1].argsort()]\n",
    "        faq_logits=faq_logits[::-1]\n",
    "\n",
    "\n",
    "        max_score=faq_logits[0,1]\n",
    "        display_num=0\n",
    "\n",
    "        for scr in faq_logits[:,1]:\n",
    "            if max_score-scr<eps and scr > minimal_score:\n",
    "                display_num+=1\n",
    "            else:\n",
    "                break\n",
    "         \n",
    "        faq_logits=faq_logits[:display_num]\n",
    "        \n",
    "        \n",
    "        return faq_logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faq(engine,question,eps,minimal_score,mode='sbert',verbose=False,plot=False):\n",
    "    \n",
    "    if mode=='fasttext':\n",
    "        vectorizer=engine.vectorize_sentence_ft\n",
    "        faq_embs=engine.ft_faq_embs\n",
    "    elif mode=='sbert':\n",
    "        vectorizer=engine.vectorize_sentence_sbert\n",
    "        faq_embs=engine.sbert_faq_embs\n",
    "            \n",
    "    question_emb=vectorizer(question)\n",
    "    faq_logits=calculate_faq(engine,question_emb,faq_embs,eps,minimal_score,mode)\n",
    "    questions_indeces=faq_logits[:,0].astype('int64').reshape((1,-1))\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print('Question: ',question)\n",
    "        print('---------------------')\n",
    "        questions=engine.faq[questions_indeces][0,:,0]\n",
    "        for i,faq_question in enumerate(questions):\n",
    "            print('index ',faq_logits[i,0],' score ', faq_logits[i,1],faq_question)\n",
    "    \n",
    "    n_clusters=2\n",
    "    if faq_logits.shape[0]>n_clusters:\n",
    "        \n",
    "        clustering=cluster.KMeans(n_clusters)\n",
    "        #clustering = cluster.OPTICS(min_samples=3, xi=.05, min_cluster_size=.05)\n",
    "     #   faq_embs=engine.ft_faq_embs\n",
    "        db_clusters = clustering.fit_predict(faq_embs[questions_indeces][0])\n",
    "\n",
    "            \n",
    "        clusters_score=np.zeros((n_clusters))\n",
    "        clusters_size=np.zeros((n_clusters))\n",
    "        \n",
    "        for i,logit in enumerate(faq_logits):\n",
    "            index=db_clusters[i]\n",
    "            clusters_score[index]+=logit[1]\n",
    "            clusters_size[index]+=1\n",
    "        \n",
    "        clusters_mean_score=clusters_score/clusters_size\n",
    "        \n",
    "        max_score_index=0\n",
    "        max_score_val=0\n",
    "        \n",
    "        for i,cluster_val in enumerate(clusters_mean_score):\n",
    "            if cluster_val> max_score_val:\n",
    "                max_score_val=cluster_val\n",
    "                max_score_index=i\n",
    "        \n",
    "        true_faq_lofits=faq_logits[np.where(db_clusters==max_score_index)]\n",
    "        \n",
    "        true_questions_indeces=true_faq_lofits[:,0].astype('int64').reshape((1,-1))\n",
    "        \n",
    "            \n",
    "        if verbose:\n",
    "            print('\\nCleaned questions')\n",
    "            print('---------------------')\n",
    "            true_questions=engine.faq[true_questions_indeces][0,:,0]\n",
    "            for i,faq_question in enumerate(true_questions):\n",
    "                print('index ',faq_logits[i,0],' score ', faq_logits[i,1],faq_question)\n",
    "                \n",
    "        \n",
    "        if plot:\n",
    "\n",
    "            umap_news=umap.UMAP()\n",
    "            question_emb=vectorizer(question).reshape(1,-1)\n",
    "            data=np.concatenate([faq_embs[questions_indeces][0],question_emb],axis=0)\n",
    "            umaped_vct=umap_news.fit_transform(data)\n",
    "            questions=engine.faq[questions_indeces][0,:,0]\n",
    "\n",
    "            myclr=ListedColormap(choice(list(sns.xkcd_rgb.values()), max(db_clusters)+1)) # Генерируем контрастную карту цветов.\n",
    "            print('количество кластеров ',max(db_clusters)+1)\n",
    "            N=15\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(N,N))\n",
    "\n",
    "            ax.scatter(umaped_vct[:-1, 0], umaped_vct[:-1, 1], s=300, c=db_clusters, cmap=myclr)\n",
    "            ax.scatter(umaped_vct[-1, 0], umaped_vct[-1, 1], s=600, c=1, cmap='rocket')\n",
    "            for i,xy in enumerate(umaped_vct[:-1]):\n",
    "                ax.text(xy[0], xy[1],'   '+str(faq_logits[i][1])+' ' +questions[i], fontsize=20,  color='black')\n",
    "\n",
    "            ax.text(umaped_vct[-1,0], umaped_vct[-1,1],'   Question: '+ question, fontsize=20,  color='black')\n",
    "            plt.show()\n",
    "        \n",
    "        return true_questions_indeces\n",
    "    else:\n",
    "        return questions_indeces\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions_diff(vectorizer,questions_indeces,min_score,max_score,verbose):\n",
    "    \n",
    "    questions=engine.faq[questions_indeces][0,:,0]\n",
    "    \n",
    "    words_pool_embs=[]\n",
    "    questions_words_embs=[]\n",
    "    questions_words=[]\n",
    "    questions_words_embs_pool=[]\n",
    "    \n",
    "    \n",
    "    for question in questions:\n",
    "        words=question.split()\n",
    "        questions_words.append(words)\n",
    "        words_embs=[vectorizer(word) for word in words]\n",
    "        questions_words_embs.append(words_embs)\n",
    "        \n",
    "        words_pool_embs=words_pool_embs+words_embs\n",
    "    \n",
    "    common_words_indx=[]\n",
    "    \n",
    "    # поиск повторяющихся слов\n",
    "    for i,word1 in enumerate(words_pool_embs):\n",
    "        word_score=0\n",
    "        \n",
    "        for j,word2 in enumerate(words_pool_embs):\n",
    "            if i!=j:\n",
    "                score=1-spatial.distance.cosine(word1, word2)\n",
    "                if score > word_score:\n",
    "                    word_score=score\n",
    "\n",
    "        if word_score>min_score:\n",
    "            common_words_indx.append(i)\n",
    "            \n",
    "    words_pool_embs=np.array(words_pool_embs)\n",
    "  #  print('common indeces ',common_words_indx)\n",
    "    common_words_embs=words_pool_embs[common_words_indx]\n",
    "  #  print(len(common_words_embs))\n",
    "  #  print(len( words_pool_embs))\n",
    "    questions_diffs_eye=[]\n",
    "    \n",
    "    # повторный проход по вопросам\n",
    "    for i,question in enumerate(questions_words_embs):\n",
    "        questions_diffs_eye.append([])\n",
    "        \n",
    "        for word1 in question:\n",
    "            word_score=0\n",
    "            for word2 in common_words_embs:\n",
    "                score=1-spatial.distance.cosine(word1, word2)\n",
    "                if score > word_score:\n",
    "                    word_score=score\n",
    "                    \n",
    "            if word_score>max_score:\n",
    "                # если слово повтторяющееся\n",
    "                questions_diffs_eye[i].append(0)\n",
    "            else:\n",
    "                # если слово уникальное\n",
    "                questions_diffs_eye[i].append(1)\n",
    "                \n",
    "    questions_diffs_eye=np.array(questions_diffs_eye)\n",
    "    if verbose:\n",
    "        \n",
    "        print('\\ndiff indeces ',questions_diffs_eye)\n",
    "        print('-----------')\n",
    "    diff_words=[]\n",
    "    \n",
    "    for i,question_eye in enumerate(questions_diffs_eye):\n",
    "        question_eye=np.array(question_eye)\n",
    "        indeces=np.where(question_eye==1)[0]\n",
    "     #   print('indeces ',indeces)\n",
    "       \n",
    "        start=indeces[0]\n",
    "        end=indeces[-1]+1\n",
    "\n",
    "        diff_words.append(questions_words[i][start:end])\n",
    "    \n",
    "    diffs= [[\" \".join(words)] for words in diff_words]\n",
    "    for i,diff in enumerate(diffs):\n",
    "        diff.append(questions_indeces[0][i])\n",
    "    \n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_search(engine,diffs_logits,answer,mode='sbert'):\n",
    "    \n",
    "    if mode=='fasttext':\n",
    "        vectorizer=engine.vectorize_sentence_ft\n",
    "        faq_embs=engine.ft_faq_embs\n",
    "    elif mode=='sbert':\n",
    "        vectorizer=engine.vectorize_sentence_sbert\n",
    "        faq_embs=engine.sbert_faq_embs\n",
    "        \n",
    "    answer_emb=vectorizer(answer)\n",
    "    diffs_embs=[vectorizer(logit[0]) for logit in diffs_logits]\n",
    "    \n",
    "    index=0\n",
    "    score=0\n",
    "    \n",
    "    for i,diff_emb in enumerate(diffs_embs):\n",
    "        diff_score=engine.cos_dist(diff_emb,answer_emb)\n",
    "        if score<diff_score:\n",
    "            index=i\n",
    "            score=diff_score\n",
    "    \n",
    "    diff_index=diffs_logits[index][1]\n",
    "    question,answer=engine.faq[diff_index]\n",
    "    \n",
    "    return question,diff_index,answer,score\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_question(engine,diffs):\n",
    "       # print('\\nQuestions difference')\n",
    "       # print('--------------')\n",
    "       # print(diffs)\n",
    "        morph = engine.MorphAnalyzer\n",
    "        system_question='Вас интересует '\n",
    "        n=len(diffs)\n",
    "\n",
    "        connectors=[]\n",
    "\n",
    "        if n==2:\n",
    "            connectors=['или ','?']\n",
    "        elif n==3:\n",
    "            connectors = [', ', ' или ','?']\n",
    "\n",
    "        for i,diff in enumerate(diffs):\n",
    "            diff_words=diff[0].split()\n",
    "            q=''\n",
    "            for word in diff_words:\n",
    "                p = morph.parse(word)[0]\n",
    "                if p.tag.POS=='NOUN':\n",
    "                    q+=p.normal_form+' '\n",
    "                else:\n",
    "                    q += word+' '\n",
    "\n",
    "            system_question+=q+connectors[i]\n",
    "            \n",
    "        return system_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sbert model and faq successfully loaded\n",
      "Passed time: 10.08 s\n"
     ]
    }
   ],
   "source": [
    "sbert_name = \"models/sbert_large_mt_nlu_ru\"\n",
    "# https://rusvectores.org/ru/models/\n",
    "#fasttext_path = 'weights/model.model'\n",
    "\n",
    "faq = pd.read_csv('faq.csv')\n",
    "engine = SearchEngine(faq.values, sbert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Как оформить льготу\n",
      "---------------------\n",
      "index  9.0  score  0.8307 Как оформить льготу в Москве?\n",
      "index  13.0  score  0.8111 Как оформить льготу в Московской области?\n",
      "index  11.0  score  0.7411 Какие документы необходимы для оформления/продления/переоформления льготы?\n",
      "\n",
      "Cleaned questions\n",
      "---------------------\n",
      "index  9.0  score  0.8307 Как оформить льготу в Москве?\n",
      "index  13.0  score  0.8111 Как оформить льготу в Московской области?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-7de2b46b31b0>:59: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  questions_diffs_eye=np.array(questions_diffs_eye)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff indeces  [list([0, 0, 0, 0, 1]) list([0, 0, 0, 0, 1, 1])]\n",
      "-----------\n",
      "Вас интересует Москве? или Московской области? ?\n",
      "москве\n",
      "Как оформить льготу в Москве? 9 0.6206793785095215\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#noise = ' пук кряк пиво интерал'\n",
    "#noise2 = ' а то я не умею'\n",
    "#question = faq['Question'][9]\n",
    "#question = 'Как оформить льготу '\n",
    "# question='льгота  от расхода'\n",
    "# question='Как будут осуществляться начисления если счетчик сломается'\n",
    "question='Как оформить льготу'\n",
    "\n",
    "eps = 0.15\n",
    "minimal_score = 0.7\n",
    "\n",
    "# ищем похожие вопросы в датасете\n",
    "# если больше 1 совпадения, кластеризуем и выбираем кластер с наибольшей точностью\n",
    "output = find_faq(engine, question, eps, minimal_score, verbose=True, plot=False)\n",
    "\n",
    "# если после кластеризации больше 1 совпадения\n",
    "if len(output[0])>1:\n",
    "\n",
    "    # ищем отличающиеся в вопросах слова\n",
    "    diffs = questions_diff(engine.vectorize_sentence_sbert, questions_indeces=output, min_score=0.8, max_score=0.8,\n",
    "                           verbose=1)\n",
    "   \n",
    "    # формируем уточняющий вопрос \n",
    "    system_question=create_system_question(engine,diffs)\n",
    "    print(system_question)\n",
    "\n",
    "    flag=True\n",
    "    # сравниеаем ответ пользователя и отличающиеся части вопросов\n",
    "    while flag:\n",
    "        answer=input()\n",
    "        question,question_index,asnwer,score = diff_search(engine, diffs, answer)\n",
    "        if score>0.6:\n",
    "            flag=False\n",
    "            print(question,question_index,score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Москве?', 9], ['Московской области?', 13]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Как оформить льготу в Москве?',\n",
       "        'Для того чтобы использовать льготу при оплате за электроэнергию, ее необходимо оформить/продлить/переоформить вАО «Мосэнергосбыт» одним из удобных способов:В\\xa0Едином личном кабинете клиента\\xa0вовкладке «Льготы» (с прикреплением необходимых документов);Воспользоваться терминалом видеосвязи (Автоматизированная система «Видеоконсультант») с операторомКонтактного центра, установленным в МФЦ. Часы работы: Пн-Пт с 08:30 до 20:00.В\\xa0любом\\xa0отделении\\xa0АО\\xa0«Мосэнергосбыт».В качестве заявителей могут выступать физические лица, проживающие в городе Москве и обладающие правом на льготы по оплате за электроэнергию в соответствии с нормативными правовыми актами Российской Федерации или города Москвы, и обратившиеся с заявлением. Интересы заявителей, могут представлять законные представители заявителей или иные лица, уполномоченные заявителем в установленном порядке.В соответствии с законодательством РФ льготы по оплате электроэнергии предоставляются гражданам не более чем на одну квартиру (жилое помещение).'],\n",
       "       ['Как оформить льготу в Московской области?',\n",
       "        'В соответствии с\\xa0законом Московской области от 23 марта 2006 г. №\\xa036/2006-ОЗ «О социальной поддержке отдельных категорий граждан в Московской области», принятым постановлением Московской областной Думы от 15 марта 2006 г. № 5/171-П, регистрация льгот в Московской области не производится, оплата за потребленную электроэнергию производится в полном объеме согласно действующим тарифам. По вопросу возмещения льгот необходимо обращаться в отделение социальной защиты Вашего муниципального района.']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.faq[output[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "noise=' пук кряк пиво интерал'\n",
    "noise2=' а то я не умею'\n",
    "question=faq['Question'][9] \n",
    "question='Как оформить льготу '\n",
    "#question='льгота  от расхода'\n",
    "#question='Как будут осуществляться начисления если счетчик сломается'\n",
    "\n",
    "\n",
    "eps=0.15\n",
    "minimal_score=0.7\n",
    "beta=1.5\n",
    "\n",
    "modes=['fasttext','sbert']\n",
    "mode=modes[1]\n",
    "\n",
    "output=find_faq(engine,question,eps,minimal_score,mode=mode,verbose=True,plot=False)\n",
    "diffs=questions_diff(engine.vectorize_sentence_sbert,questions_indeces=output,min_score=0.8,max_score=0.8,verbose=0)\n",
    "print('\\nQuestions difference')\n",
    "print('--------------')\n",
    "print(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "answer='московской области'\n",
    "question=diff_search(engine,diffs,answer,mode)\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect1=engine.vectorize_sentence_sbert('москва')\n",
    "vect2=engine.vectorize_sentence_sbert('московская область')\n",
    "vect3=engine.vectorize_sentence_sbert('город')\n",
    "engine.cos_dist(vect1,vect3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse('области')[0].normal_form\n",
    "# ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=['Сборка простой люстры',\n",
    "'Сборка сложной люстры',\n",
    "'Установка простой люстры',\n",
    "'Установка сложной люстры',\n",
    "'Установка светильника типа Армстронг',\n",
    "'Установка светильника настенного, бра',\n",
    "'Установка точечного светильника',\n",
    "'Подключение светильника Выход',\n",
    "'Подключени трансформатора для галогенных ламп',\n",
    "'Установка антивандального светильника']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=questions[:2]\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2=questions[4:7]\n",
    "q2_words=[]\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "for q in q2:\n",
    "    words=q.split()\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0] \n",
    "        q2_words.append(p.normal_form)\n",
    "        \n",
    "q2_words=set(q2_words)\n",
    "q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word = \"светильником\"\n",
    "p = morph.parse(word)[0]  # Делаем полный разбор, и берем первый вариант разбора (условно \"самый вероятный\", но не факт что правильный)\n",
    "print(p.normal_form)  # стать"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
