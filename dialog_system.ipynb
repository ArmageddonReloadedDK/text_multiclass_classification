{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\torch\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models import FastText,KeyedVectors,fasttext\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "'''import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape,BatchNormalization,MaxPooling2D,Lambda\n",
    "from tensorflow.keras.layers import Dense,Activation,Reshape,Conv2D,LeakyReLU,concatenate,Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faq(urls,question_data,answer_data):\n",
    "    #\n",
    "    #  urls - list [urls1,urls2,..]\n",
    "    #  question_data, answer_data - list [tag_value,class_value]  \n",
    "    #\n",
    "    #  return faq - list shape (n,2), where [:,0] - question, [:,1] - answer\n",
    "    #\n",
    "    question_tag,question_class=question_data\n",
    "    answer_tag,answer_class=answer_data\n",
    "    \n",
    "    rs =[ requests.get(url) for url in urls]\n",
    "    responses = [r.text.encode('utf-8') for r in rs]\n",
    "    soups = [BeautifulSoup(response) for response in responses]\n",
    "\n",
    "    faq=[]\n",
    "    for soup in soups:\n",
    "        question=soup.findAll(question_tag, class_=question_class)\n",
    "        answer=soup.findAll(answer_tag,class_=answer_class)\n",
    "        temp_questions=[]\n",
    "        temp_answers=[]\n",
    "        for q in question:\n",
    "            txt=str(q.text)\n",
    "            txt=txt.replace('  ','')\n",
    "            txt=txt.replace('\"','')\n",
    "            txt=txt.replace('\t','')\n",
    "            txt=txt.replace('\\n','')\n",
    "            temp_questions.append(txt)\n",
    "        for ans in answer:\n",
    "            txt=str(ans.text)\n",
    "            txt=txt.replace('  ','')\n",
    "            txt=txt.replace('\t','')\n",
    "            txt=txt.replace('\\n','')\n",
    "            temp_answers.append(txt)\n",
    "        \n",
    "        temp_questions=np.array(temp_questions).reshape((-1,1))\n",
    "        temp_answers=np.array(temp_answers).reshape((-1,1))\n",
    "        temp_faq=np.concatenate([temp_questions,temp_answers],axis=1)\n",
    "        faq.append(temp_faq)\n",
    "            \n",
    "\n",
    "    return faq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls1=['https://www.mosenergosbyt.ru/common/lobby/questions/category_5749.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5739.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_48100.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_48101.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_48102.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_48103.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5745.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5763.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5754.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5716.php']\n",
    "\n",
    "urls2=['https://mosoblgaz.ru/company/query_answer/']\n",
    "\n",
    "faq1=get_faq(urls1,['button','btn btn-link collapsed'],['div','collapse'])\n",
    "faq2=get_faq(urls2,['div','faq-list-item__title js-accordion-item-title'],\n",
    "             ['div','faq-list-item__text-inner js-accordion-item-inner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for document in faq1:\n",
    "    for line in document:\n",
    "        df.append(line)\n",
    "    \n",
    "\n",
    "df=pd.DataFrame(df,columns = ['Question','Answer'])\n",
    "df.to_csv('faq.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Как изменить ФИО собственника?</td>\n",
       "      <td>Для переоформления лицевого счета на нового вл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Как исправить ошибку (или опечатку) в ФИОсобст...</td>\n",
       "      <td>Подать заявку на изменение данных можно в личн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Как изменить телефон?</td>\n",
       "      <td>Изменить номер мобильного телефона можно самос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Как изменить количество проживающих?</td>\n",
       "      <td>Изменить данные о количестве проживающих можно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Как изменить количество комнат?</td>\n",
       "      <td>Изменить данные о количестве комнат можно толь...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0                     Как изменить ФИО собственника?   \n",
       "1  Как исправить ошибку (или опечатку) в ФИОсобст...   \n",
       "2                              Как изменить телефон?   \n",
       "3               Как изменить количество проживающих?   \n",
       "4                    Как изменить количество комнат?   \n",
       "\n",
       "                                              Answer  \n",
       "0  Для переоформления лицевого счета на нового вл...  \n",
       "1  Подать заявку на изменение данных можно в личн...  \n",
       "2  Изменить номер мобильного телефона можно самос...  \n",
       "3  Изменить данные о количестве проживающих можно...  \n",
       "4  Изменить данные о количестве комнат можно толь...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq=pd.read_csv('faq.csv')\n",
    "faq.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine():\n",
    "    def __init__(self,faq,sbert_name=False,fasttext_path=False):\n",
    "        self.faq=faq\n",
    "        if sbert_name:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            self.sbert_model = AutoModel.from_pretrained(sbert_name)\n",
    "            self.sbert_model.to(self.device)\n",
    "            self.sbert_tokenizer = AutoTokenizer.from_pretrained(sbert_name)\n",
    "            self.sbert_faq_embs=np.array([self.vectorize_sentence_sbert(sent) for sent in self.faq[:,0]])\n",
    "            \n",
    "        if fasttext_path:\n",
    "            self.ft_model= fasttext.FastTextKeyedVectors.load(fasttext_path)\n",
    "            self.ft_faq_embs=np.array([self.vectorize_sentence_ft(sent) for sent in self.faq[:,0]])\n",
    "        \n",
    "    def vectorize_sentence_ft(self,sentence):\n",
    "        txt=sentence.split()\n",
    "        val=0\n",
    "        for word in txt:\n",
    "            val+=self.ft_model[word]\n",
    "\n",
    "        return val/len(txt)\n",
    "    \n",
    "\n",
    "    def vectorize_sentence_sbert(self,sentence):\n",
    "    \n",
    "        encoded_input = self.sbert_tokenizer(sentence,\n",
    "                                             padding=True,\n",
    "                                             truncation=True,\n",
    "                                             max_length=24,\n",
    "                                             return_tensors='pt').to(self.device)\n",
    "\n",
    "   #     with torch.no_grad():\n",
    "        model_output = self.sbert_model(**encoded_input)\n",
    "            \n",
    "        #Perform pooling. In this case, mean pooling\n",
    "        sentence_embedding = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embedding = np.squeeze(sentence_embedding)\n",
    "        \n",
    "        return sentence_embedding.cpu().data.numpy()\n",
    "    \n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def mean_pooling(cls,model_output, attention_mask):\n",
    "        #Mean Pooling - Take attention mask into account for correct averaging\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def search_faq(self,question,eps,minimal_score,mode):\n",
    "        if mode=='fasttext':\n",
    "            vectorizers=[self.vectorize_sentence_ft]\n",
    "            faq_embs=[self.ft_faq_embs]\n",
    "        elif mode=='sbert':\n",
    "            vectorizers=[self.vectorize_sentence_sbert]\n",
    "            faq_embs=[self.sbert_faq_embs]\n",
    "        elif mode=='both':\n",
    "            vectorizers=[self.vectorize_sentence_ft,self.vectorize_sentence_sbert]\n",
    "            faq_embs=[self.ft_faq_embs,self.sbert_faq_embs]\n",
    "            \n",
    "            \n",
    "        question_emb=[vectorizer(question) for vectorizer in vectorizers]\n",
    "        score=np.zeros((self.faq.shape[0],len(vectorizers)))\n",
    "\n",
    "        for i in range(len(self.faq)):\n",
    "            faq_emb=[]\n",
    "            for f in faq_embs:\n",
    "                faq_emb.append(f[i])\n",
    "            \n",
    "            for j in range(len(vectorizers)):\n",
    "                \n",
    "                distance=1-spatial.distance.cosine( faq_emb[j],question_emb[j])\n",
    "                score[i,j]=distance\n",
    "                \n",
    "        if mode=='both':\n",
    "            new_score=np.zeros((faq.shape[0],1))\n",
    "            temp_score=score.copy()\n",
    "            for i,scr in enumerate(temp_score):\n",
    "                n=len(scr)\n",
    "                mul=1\n",
    "                for val in scr:\n",
    "                    mul*=val\n",
    "                new_score[i]=mul**(1/n)\n",
    "            del(score)\n",
    "            score=new_score.copy()\n",
    "            \n",
    "        \n",
    "        faq_and_score=np.concatenate([faq,score],axis=1)\n",
    "        faq_and_score=faq_and_score[faq_and_score[:, 2].argsort()]\n",
    "        faq_and_score=faq_and_score[::-1]\n",
    "\n",
    "\n",
    "        max_score=faq_and_score[0,2]\n",
    "        display_num=0\n",
    "\n",
    "        for scr in faq_and_score[:,2]:\n",
    "            if max_score-scr<eps and scr > minimal_score:\n",
    "                display_num+=1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return faq_and_score[:display_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sbert_name=\"sberbank-ai/sbert_large_mt_nlu_ru\"\n",
    "# https://rusvectores.org/ru/models/\n",
    "fasttext_path='214/model.model'\n",
    "engine=SearchEngine(faq.values,sbert_name,fasttext_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=vectorizer_obj.vectorize_sentence_sbert\n",
    "eps=0.2\n",
    "minimal_score=0.6\n",
    "\n",
    "faq_embs=np.array([vectorizer(sent) for sent in faq.values[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Как отключить «Автоматический платеж»? пук кряк пиво интерал Автоматический платеж\n",
      "---------------------\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question=faq['Question'][70]+' пук кряк пиво интерал Автоматический платеж'\n",
    "#question=faq['Question'][70]+faq['Question'][71]\n",
    "print('Question: ',question)\n",
    "print('---------------------')\n",
    "\n",
    "\n",
    "#search_faq(vectorizer,question,faq.values,faq_embs,eps,minimal_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  В какой срок выполняется заявка? а то я не умею\n",
      "---------------------\n",
      "[['В какой срок выполняется заявка?'\n",
      "  'В соответствии с требованиями Федерального закона от 27.12.2018 № 522-ФЗ «О внесении изменений в отдельные законодательные акты Российской Федерации в связи с развитием систем учёта электрической энергии (мощности) в Российской Федерации» и постановления Правительстваот 29.06.2020 № 950 «О внесении изменений в некоторые акты Правительства Российской Федерации по вопросам совершенствования организации учёта электрической энергии» с 1 июля 2020 года обязанность по замене неисправных приборов учёта электроэнергии или счётчиков электроэнергии с истекшим межповерочным интервалом, в жилых помещениях многоквартирных жилых домов возложена на гарантирующего поставщика электроэнергии. В Москве и Московской области гарантирующим поставщиком электроэнергии является АО «Мосэнергосбыт». В индивидуальных жилых домах эта обязанность возложена на сетевые организации ПАО «Россети Московский регион». Замена приборов учёта будет осуществляться в следующие сроки:Если по состоянию на 01.04.2020 или ранее у потребителя отсутствовал ПУ/истёк срок его эксплуатации/вышел из строя, то установка (замена) ПУ осуществляется до 31.12.2023.Если по состоянию на 01.04.2020 или ранее у потребителя истёк МПИ ПУ, то установка (замена) со стороны ГП осуществляется до 31.12.2021.В иных случаях – установка (замена) должна быть осуществлена в течение 6 месяцев.График замены приборов учёта в многоквартирных жилых домах Москвы и Московской области можно посмотреть на сайте АО «Мосэнергосбыт».В остальных случаях замена/установка прибора учёта осуществляется на платной основе. Заявка выполняется в срок, не превышающий 10 рабочих дней с момента поступления оплаты на расчётный счёт АО «Мосэнергосбыт». После поступления оплаты с клиентом связывается мастер (сотрудник АО «Мосэнергосбыт») для уточнения времени проведения работ.'\n",
      "  0.7899765968322754]]\n",
      "---------------------\n",
      "Cleaned question: В какой срок выполняется заявка?\n",
      "---------------------\n",
      "Wall time: 28.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "noise=' пук кряк пиво интерал'\n",
    "noise2=' а то я не умею'\n",
    "question=faq['Question'][50] +noise2\n",
    "#question='Как оформить льготу'\n",
    "print('Question: ',question)\n",
    "print('---------------------')\n",
    "\n",
    "eps=0.4\n",
    "minimal_score=0.5\n",
    "\n",
    "mode='sbert'\n",
    "\n",
    "output=engine.search_faq(question,eps,minimal_score,mode=mode)\n",
    "\n",
    "cleaned_question=clean_question(vectorizer_obj.vectorize_sentence_ft,question,output[:,0],min_val)\n",
    "print(output)\n",
    "print('---------------------')\n",
    "print('Cleaned question:',cleaned_question)\n",
    "print('---------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question(vectorizer,question,pred_questions,min_score):\n",
    "    question_words=question.split()\n",
    "    question_words_embs=[vectorizer(word) for word in question_words]\n",
    "    \n",
    "    pred_question_words=[]\n",
    "    for pred_question in pred_questions:\n",
    "        words=pred_question.split()\n",
    "        pred_question_words=pred_question_words+words\n",
    "    \n",
    "    pred_question_words_embs=[vectorizer(word) for word in pred_question_words]\n",
    "    \n",
    "    dlt_inx=[]\n",
    "    \n",
    "    for i,question_word_emb in enumerate(question_words_embs):\n",
    "        word_score=0\n",
    "        for j,pred_question_word_emb in enumerate(pred_question_words_embs):\n",
    "            score=1-spatial.distance.cosine(question_word_emb, pred_question_word_emb)\n",
    "            if score > word_score:\n",
    "                word_score=score\n",
    "                \n",
    "        if word_score<min_score:\n",
    "            dlt_inx.append(i)\n",
    "            \n",
    "    for i in reversed(dlt_inx):\n",
    "        question_words.pop(i)\n",
    "    \n",
    "    return \" \".join(question_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search engine\n",
    "\n",
    "FastText documents embeddings vs questions\n",
    "\n",
    "document=questions+answers on one theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[]\n",
    "documents=[]\n",
    "class_num=len(faq1)\n",
    "questions2class=dict()\n",
    "questions_classes=[]\n",
    "\n",
    "\n",
    "\n",
    "for i,document in enumerate(faq1):\n",
    "    \n",
    "    temp_questions=[]\n",
    "    for line in document:\n",
    "        question=line[0]\n",
    "        temp_questions.append(question)\n",
    "        questions2class[question]=i\n",
    "    questions.append(temp_questions)\n",
    "        \n",
    "        \n",
    "    temp_documents=[]\n",
    "    for line in document:\n",
    "        temp_documents.append(line[0])\n",
    "        temp_documents.append(line[1])\n",
    "\n",
    "        \n",
    "    documents.append(temp_documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_embs=np.zeros((len(documents),300))\n",
    "for i,document in enumerate(documents):\n",
    "    documents_embs[i]=sentence_embedding(ft_model,document)\n",
    "    \n",
    "questions_embs=[]\n",
    "for i,question_document in enumerate(questions):\n",
    "    temp_questions_embs=[]\n",
    "    for question in question_document:\n",
    "        temp_questions_embs.append(sentence_embedding(ft_model,[question]))\n",
    "    questions_embs.append(temp_questions_embs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,document in enumerate(documents_embs):\n",
    "\n",
    "    pairs_number=len(questions_embs[i])\n",
    "    pairs=[]\n",
    "    labels=[]\n",
    "    #  true pairs\n",
    "    for question in questions_embs[i]:\n",
    "        pairs.append([document,question])\n",
    "        labels.append(1)\n",
    "        \n",
    "    # false document\n",
    "    for j in range(pairs_number):\n",
    "        flag=True\n",
    "        rnd_indx=np.random.randint(0,len(documents_embs))\n",
    "        while  rnd_indx==i:\n",
    "                rnd_indx=np.random.randint(0,len(documents_embs))\n",
    "        pairs.append([documents_embs[rnd_indx],question])\n",
    "        labels.append(0)\n",
    "        \n",
    "    # false questions\n",
    "    for j in range(pairs_number):\n",
    "        flag=True\n",
    "        rnd_indx=np.random.randint(0,len(documents_embs))\n",
    "        while rnd_indx==i:\n",
    "                rnd_indx=np.random.randint(0,len(documents_embs))\n",
    "                \n",
    "        rnd_indx_question=np.random.randint(0,len(questions_embs[rnd_indx]))\n",
    "        pairs.append([document,questions_embs[rnd_indx][rnd_indx_question]])\n",
    "        labels.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=np.array(pairs).astype('float32')\n",
    "labels=np.array(labels).astype('float32')\n",
    "x_train,x_test,y_train,y_test=train_test_split(pairs,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss_with_margin(margin):\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "    \n",
    "        square_pred = K.square(y_pred)\n",
    "        margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "        return K.mean( (1 - y_true) * square_pred+y_true*margin_square)\n",
    "    \n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    #\n",
    "    #   it is also good idea to use VGG model, but it requires powerfull GPU\n",
    "    #   model = Sequential(VGG16(weights='imagenet', include_top=False, input_shape=image_shape).layers)\n",
    "    #\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Reshape((15,20,1),input_shape=emb_shape))\n",
    "#    model.add(Conv2D(32,(3,3)))\n",
    "#    model.add(BatchNormalization())\n",
    "        \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_shape=(300,)\n",
    "base_network = get_model()\n",
    "\n",
    "input_a = Input(shape=emb_shape)\n",
    "vect_output_a = base_network(input_a)\n",
    "\n",
    "input_b = Input(shape=emb_shape)\n",
    "vect_output_b = base_network(input_b)\n",
    "\n",
    "x = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([vect_output_a, vect_output_b])\n",
    "output= Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = Model([input_a, input_b], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = RMSprop(  learning_rate=0.015)\n",
    "#optim = Adam(  learning_rate=0.015)\n",
    "model.compile(loss=contrastive_loss_with_margin(margin=1), optimizer=optim)\n",
    "history = model.fit([x_train[:,0],x_train[:,1]], \n",
    "                    y_train, \n",
    "                    epochs=30, \n",
    "                    batch_size=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() > 0.5\n",
    "    return round(np.mean(pred == y_true),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = round(model.evaluate(x=[x_test[:,0],x_test[:,1]], y=y_test),4)\n",
    "\n",
    "y_pred_train = model.predict([x_train[:,0],x_train[:,1]])\n",
    "train_accuracy = compute_accuracy(y_train, y_pred_train)\n",
    "\n",
    "y_pred_test = model.predict([x_test[:,0], x_test[:,1]])\n",
    "test_accuracy = compute_accuracy(y_test, y_pred_test)\n",
    "\n",
    "print(\"Loss = {}, Train Accuracy = {} Test Accuracy = {}\".format(loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model, configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "[Reference](https://medium.com/swlh/fine-tuning-bert-for-text-classification-and-question-answering-using-tensorflow-framework-4d09daeb3330#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjFiZjhhODRkM2VjZDc3ZTlmMmFkNWYwNmZmZDI2MDcwMWRkMDZkOTAiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MjU5MDYxNDIsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwNjU4MDUxNjE5NzU3NTk0MTk2NCIsImhkIjoibWllbS5oc2UucnUiLCJlbWFpbCI6ImRna2FncmFtYW55YW5AbWllbS5oc2UucnUiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiYXpwIjoiMjE2Mjk2MDM1ODM0LWsxazZxZTA2MHMydHAyYTJqYW00bGpkY21zMDBzdHRnLmFwcHMuZ29vZ2xldXNlcmNvbnRlbnQuY29tIiwibmFtZSI6ItCU0LDQstC40LQg0JrQsNCz0YDQsNC80LDQvdGP0L0iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EtL0FPaDE0R2hGdEZGSkZMaDlyUFV0QXdZLXlyZWlRczJObjEyQ2xpc3hmX3hQPXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6ItCU0LDQstC40LQiLCJmYW1pbHlfbmFtZSI6ItCa0LDQs9GA0LDQvNCw0L3Rj9C9IiwiaWF0IjoxNjI1OTA2NDQyLCJleHAiOjE2MjU5MTAwNDIsImp0aSI6IjhiYmVjYmRjYWRiZTAyZDZkNjQ1MTMxY2Q2NzJhZTI4MWViMTdkNWYifQ.LRr773uwpuGgpYnO7WsltRMVbC3JdFg7DBIlPtnLhN11OpKyNZ5X3y2ZGLx72_tJaSXZS52xdLcEsLuM4Tk1Ta_4ifnuAMkkeJMYJct0DpJOqXGYWS9S2atl8JLvbQaLyNrseNHneLtzRgtj--Htk1Lq0red-VmGxO849tzhKfjfQDPw-PsKxnhKMxHoEHJs91z-djL_L8ATQ6p86TcSWzWGsi4Ya69TmkMiRw-W2eMaZjN11gjSXUCnJKUUC122fluOmDzsGnUbtpoSGD-86mTa368FTFLbQSC7M8SGiZKo_pIZCcKrGjBQM-bJFAMs3-I8J7nM6782Vlh_CZiBew)\n",
    "\n",
    "[просто ссылка с хорошей инфой]( https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(context,questions,answer):\n",
    "    \n",
    "    t_context = tokenizer(context,padding=True)\n",
    "    t_questions = tokenizer(questions,padding=True)\n",
    "        \n",
    "    # create inputs as usual\n",
    "    input_ids = t_context['input_ids'] + t_questions['input_ids']\n",
    "    token_type_ids = [0] * len(t_context['input_ids']) + t_questions['attention_mask']\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    return input_ids,token_type_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=list(faq1[:,1].reshape((1,-1))[0])\n",
    "context=\" \".join(questions)\n",
    "input_ids,token_type_ids,attention_mask=preprocess(context,questions,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(context.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=questions[0].replace('\\n','')\n",
    "len(string.replace('/','').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "        self.start_token_idx = -1\n",
    "        self.end_token_idx = -1\n",
    "\n",
    "    def preprocess(self):\n",
    "        # clean context and question\n",
    "        context = \" \".join(str(self.context).split())\n",
    "        question = \" \".join(str(self.question).split())\n",
    "        # tokenize context and question\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "        \n",
    "        # if this is validation or training sample, preprocess answer\n",
    "        if self.answer_text is not None:\n",
    "            answer = \" \".join(str(self.answer_text).split())\n",
    "            # check if end character index is in the context\n",
    "            end_char_idx = self.start_char_idx + len(answer)\n",
    "            if end_char_idx >= len(context):\n",
    "                self.skip = True\n",
    "                return\n",
    "            # mark all the character indexes in context that are also in answer     \n",
    "            is_char_in_ans = [0] * len(context)\n",
    "            for idx in range(self.start_char_idx, end_char_idx):\n",
    "                is_char_in_ans[idx] = 1\n",
    "            ans_token_idx = []\n",
    "            # find all the tokens that are in the answers\n",
    "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "                if sum(is_char_in_ans[start:end]) > 0:\n",
    "                    ans_token_idx.append(idx)\n",
    "            if len(ans_token_idx) == 0:\n",
    "                self.skip = True\n",
    "                return\n",
    "            # get start and end token indexes\n",
    "            self.start_token_idx = ans_token_idx[0]\n",
    "            self.end_token_idx = ans_token_idx[-1]\n",
    "        # create inputs as usual\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        # add padding if necessary\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        self.input_word_ids = input_ids\n",
    "        self.input_type_ids = token_type_ids\n",
    "        self.input_mask = attention_mask\n",
    "        self.context_token_to_char = tokenized_context.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_squad_examples(raw_data):\n",
    "    squad_examples = []\n",
    "    for item in raw_data[\"data\"]:\n",
    "        for para in item[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                if \"answers\" in qa:\n",
    "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
    "                    start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
    "                    squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\n",
    "                else:\n",
    "                    squad_eg = Sample(question, context)\n",
    "                squad_eg.preprocess()\n",
    "                squad_examples.append(squad_eg)\n",
    "    return squad_examples\n",
    "\n",
    "\n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        \"input_word_ids\": [],\n",
    "        \"input_type_ids\": [],\n",
    "        \"input_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "        if item.skip == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "    x = [dataset_dict[\"input_word_ids\"],\n",
    "         dataset_dict[\"input_mask\"],\n",
    "         dataset_dict[\"input_type_ids\"]]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        # remove redundant whitespaces\n",
    "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
    "        # remove articles\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        text = re.sub(regex, \" \", text)\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # get the offsets of the first and last tokens of predicted answers\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        # for every pair of offsets\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            # take the required Sample object with the ground-truth answers in it\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            # use offsets to get back the span of text corresponding to\n",
    "            # our predicted first and last tokens\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "            normalized_pred_ans = self.normalize_text(pred_ans)\n",
    "            # clean the real answers\n",
    "            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            # check if the predicted answer is in an array of the ground-truth answers\n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
    "eval_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
    "\n",
    "with open(train_path) as f:\n",
    "    raw_train_data = json.load(f)\n",
    "    \n",
    "with open(eval_path) as f:\n",
    "    raw_eval_data = json.load(f)\n",
    "\n",
    "max_seq_length = 384\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
    "train_squad_examples = create_squad_examples(raw_train_data)\n",
    "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n",
    "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
    "start_logits = layers.Flatten()(start_logits)\n",
    "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
    "end_logits = layers.Flatten()(end_logits)\n",
    "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=8, callbacks=[ValidationCallback(x_eval, y_eval)])\n",
    "model.save_weights(\"./weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"data\":\n",
    "    [\n",
    "        {\"title\": \"Project Apollo\",\n",
    "         \"paragraphs\": [\n",
    "             {\n",
    "                 \"context\": \"The Apollo program, also known as Project Apollo, was the third United States human \"\n",
    "                            \"spaceflight program carried out by the National Aeronautics and Space Administration (\"\n",
    "                            \"NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First \"\n",
    "                            \"conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to \"\n",
    "                            \"follow the one-man Project Mercury which put the first Americans in space, Apollo was \"\n",
    "                            \"later dedicated to President John F. Kennedy's national goal of landing a man on the \"\n",
    "                            \"Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in \"\n",
    "                            \"a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project \"\n",
    "                            \"Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, \"\n",
    "                            \"and was supported by the two man Gemini program which ran concurrently with it from 1962 \"\n",
    "                            \"to 1966. Gemini missions developed some of the space travel techniques that were \"\n",
    "                            \"necessary for the success of the Apollo missions. Apollo used Saturn family rockets as \"\n",
    "                            \"launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications \"\n",
    "                            \"Program, which consisted of Skylab, a space station that supported three manned missions \"\n",
    "                            \"in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the \"\n",
    "                            \"Soviet Union in 1975.\",\n",
    "                 \"qas\": [\n",
    "                     {\"question\": \"What project put the first Americans into space?\",\n",
    "                      \"id\": \"Q1\"\n",
    "                      },\n",
    "                     {\"question\": \"What program was created to carry out these projects and missions?\",\n",
    "                      \"id\": \"Q2\"\n",
    "                      },\n",
    "                     {\"question\": \"What year did the first manned Apollo flight occur?\",\n",
    "                      \"id\": \"Q3\"\n",
    "                      },\n",
    "                     {\"question\": \"What President is credited with the original notion of putting Americans in space?\",\n",
    "                      \"id\": \"Q4\"\n",
    "                      },\n",
    "                     {\"question\": \"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\",\n",
    "                      \"id\": \"Q5\"\n",
    "                      },\n",
    "                     {\"question\": \"How long did Project Apollo run?\",\n",
    "                      \"id\": \"Q6\"\n",
    "                      },\n",
    "                     {\"question\": \"What program helped develop space travel techniques that Project Apollo used?\",\n",
    "                      \"id\": \"Q7\"\n",
    "                      },\n",
    "                     {\"question\": \"What space station supported three manned missions in 1973-1974?\",\n",
    "                      \"id\": \"Q8\"\n",
    "                      }\n",
    "                 ]}]}]}\n",
    "\n",
    "test_samples = create_squad_examples(data)\n",
    "x_test, _ = create_inputs_targets(test_samples)\n",
    "pred_start, pred_end = model.predict(x_test)\n",
    "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "    test_sample = test_samples[idx]\n",
    "    offsets = test_sample.context_token_to_char\n",
    "    start = np.argmax(start)\n",
    "    end = np.argmax(end)\n",
    "    pred_ans = None\n",
    "    if start >= len(offsets):\n",
    "        continue\n",
    "    pred_char_start = offsets[start][0]\n",
    "    if end < len(offsets):\n",
    "        pred_ans = test_sample.context[pred_char_start:offsets[end][1]]\n",
    "    else:\n",
    "        pred_ans = test_sample.context[pred_char_start:]\n",
    "    print(\"Q: \" + test_sample.question)\n",
    "    print(\"A: \" + pred_ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
