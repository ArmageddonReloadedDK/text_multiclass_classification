{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models import FastText,KeyedVectors,fasttext\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "#import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape,BatchNormalization,MaxPooling2D,Lambda\n",
    "from tensorflow.keras.layers import Dense,Activation,Reshape,Conv2D,LeakyReLU,concatenate,Flatten\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faq(urls,question_data,answer_data):\n",
    "    #\n",
    "    #  urls - list [urls1,urls2,..]\n",
    "    #  question_data, answer_data - list [tag_value,class_value]  \n",
    "    #\n",
    "    #  return faq - list shape (n,2), where [:,0] - question, [:,1] - answer\n",
    "    #\n",
    "    question_tag,question_class=question_data\n",
    "    answer_tag,answer_class=answer_data\n",
    "    \n",
    "    rs =[ requests.get(url) for url in urls]\n",
    "    responses = [r.text.encode('utf-8') for r in rs]\n",
    "    soups = [BeautifulSoup(response) for response in responses]\n",
    "\n",
    "    faq=[]\n",
    "    for soup in soups:\n",
    "        question=soup.findAll(question_tag, class_=question_class)\n",
    "        answer=soup.findAll(answer_tag,class_=answer_class)\n",
    "        temp_questions=[]\n",
    "        temp_answers=[]\n",
    "        for q in question:\n",
    "            txt=str(q.text)\n",
    "            txt=txt.replace('  ','')\n",
    "            txt=txt.replace('\"','')\n",
    "            txt=txt.replace('\t','')\n",
    "            txt=txt.replace('\\n','')\n",
    "            temp_questions.append(txt)\n",
    "        for ans in answer:\n",
    "            txt=str(ans.text)\n",
    "            txt=txt.replace('  ','')\n",
    "            txt=txt.replace('\t','')\n",
    "            txt=txt.replace('\\n','')\n",
    "            temp_answers.append(txt)\n",
    "        \n",
    "        temp_questions=np.array(temp_questions).reshape((-1,1))\n",
    "        temp_answers=np.array(temp_answers).reshape((-1,1))\n",
    "        temp_faq=np.concatenate([temp_questions,temp_answers],axis=1)\n",
    "        faq.append(temp_faq)\n",
    "            \n",
    "\n",
    "    return faq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls1=['https://www.mosenergosbyt.ru/common/lobby/questions/category_5749.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5739.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_48100.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_48101.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_48102.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_48103.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5745.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5763.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5754.php',\n",
    "     'https://www.mosenergosbyt.ru/common/lobby/questions/category_5716.php']\n",
    "\n",
    "urls2=['https://mosoblgaz.ru/company/query_answer/']\n",
    "\n",
    "faq1=get_faq(urls1,['button','btn btn-link collapsed'],['div','collapse'])\n",
    "faq2=get_faq(urls2,['div','faq-list-item__title js-accordion-item-title'],\n",
    "             ['div','faq-list-item__text-inner js-accordion-item-inner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for document in faq1:\n",
    "    for line in document:\n",
    "        df.append(line)\n",
    "    \n",
    "\n",
    "df=pd.DataFrame(df,columns = ['Question','Answer'])\n",
    "df.to_csv('faq.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Как изменить ФИО собственника?</td>\n",
       "      <td>Для переоформления лицевого счета на нового вл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Как исправить ошибку (или опечатку) в ФИОсобст...</td>\n",
       "      <td>Подать заявку на изменение данных можно в личн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Как изменить телефон?</td>\n",
       "      <td>Изменить номер мобильного телефона можно самос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Как изменить количество проживающих?</td>\n",
       "      <td>Изменить данные о количестве проживающих можно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Как изменить количество комнат?</td>\n",
       "      <td>Изменить данные о количестве комнат можно толь...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0                     Как изменить ФИО собственника?   \n",
       "1  Как исправить ошибку (или опечатку) в ФИОсобст...   \n",
       "2                              Как изменить телефон?   \n",
       "3               Как изменить количество проживающих?   \n",
       "4                    Как изменить количество комнат?   \n",
       "\n",
       "                                              Answer  \n",
       "0  Для переоформления лицевого счета на нового вл...  \n",
       "1  Подать заявку на изменение данных можно в личн...  \n",
       "2  Изменить номер мобильного телефона можно самос...  \n",
       "3  Изменить данные о количестве проживающих можно...  \n",
       "4  Изменить данные о количестве комнат можно толь...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq=pd.read_csv('faq.csv')\n",
    "faq.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model= fasttext.FastTextKeyedVectors.load('214/model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(model,sentence):\n",
    "    txt=sentence.split()\n",
    "    val=0\n",
    "    for word in txt:\n",
    "        val+=model[word]\n",
    "\n",
    "    return val/len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_embs=np.array([sentence_embedding(emb_model,sent) for sent in faq.values[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faq(emb_model,question,faq,faq_embs,eps,minimal_score):\n",
    "    question_emb=sentence_embedding(emb_model,question)\n",
    "    score=np.zeros((faq.shape[0],1))\n",
    "    \n",
    "    for i,faq_emb in enumerate(faq_embs):\n",
    "        score[i,0]=1-spatial.distance.cosine(question_emb, faq_emb)\n",
    "    \n",
    "    faq_and_score=np.concatenate([faq,score],axis=1)\n",
    "    faq_and_score=faq_and_score[faq_and_score[:, 2].argsort()]\n",
    "    faq_and_score=faq_and_score[::-1]\n",
    "    \n",
    "    max_score=faq_and_score[0,2]\n",
    "    display_num=0\n",
    "    \n",
    "    for scr in faq_and_score[:,2]:\n",
    "        if max_score-scr<eps and scr > minimal_score:\n",
    "            display_num+=1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return faq_and_score[:display_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Как рассчитывается оплата за электроэнергию? пук кряк пиво интерал\n",
      "---------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['Как рассчитывается оплата за электроэнергию?',\n",
       "        'Сумма к оплате рассчитывается одним из следующих способов:На основании переданных Вами показаний прибора учёта;\\xa0При условии, что показания счетчика были переданы в период с 15 по 26 число включительноНа основании показаний прибора учёта, снятых сотрудниками АО «Мосэнергосбыт»;Контрольное снятие показаний производится сотрудником АО «Мосэнергосбыт» 2 раза в год.При отсутствии показаний в расчётном периоде сумма к оплате рассчитывается на основании среднемесячного объёма потребления;При отсутствии информации о среднемесячном объёме потребления расчёт производится на основании норматива потребления.',\n",
       "        0.8685539960861206],\n",
       "       ['Когда будет включена электроэнергия, отключенная за долги, если произведена оплата задолженности и за подключение?',\n",
       "        'Предоставление коммунальной услуги по электроснабжению возобновляется в течение 2 календарных дней со дня полного погашения задолженности и оплаты расходов исполнителя по введению ограничения, приостановлению и возобновлению предоставления коммунальной услуги.',\n",
       "        0.7868824601173401]], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=faq['Question'][34]+' пук кряк пиво интерал'\n",
    "print('Question: ',question)\n",
    "print('---------------------')\n",
    "eps=0.1\n",
    "minimal_score=0.6\n",
    "search_faq(emb_model,question,faq.values,faq_embs,eps,minimal_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search engine\n",
    "\n",
    "FastText documents embeddings vs questions\n",
    "\n",
    "document=questions+answers on one theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[]\n",
    "documents=[]\n",
    "class_num=len(faq1)\n",
    "questions2class=dict()\n",
    "questions_classes=[]\n",
    "\n",
    "\n",
    "\n",
    "for i,document in enumerate(faq1):\n",
    "    \n",
    "    temp_questions=[]\n",
    "    for line in document:\n",
    "        question=line[0]\n",
    "        temp_questions.append(question)\n",
    "        questions2class[question]=i\n",
    "    questions.append(temp_questions)\n",
    "        \n",
    "        \n",
    "    temp_documents=[]\n",
    "    for line in document:\n",
    "        temp_documents.append(line[0])\n",
    "        temp_documents.append(line[1])\n",
    "\n",
    "        \n",
    "    documents.append(temp_documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_embs=np.zeros((len(documents),300))\n",
    "for i,document in enumerate(documents):\n",
    "    documents_embs[i]=sentence_embedding(ft_model,document)\n",
    "    \n",
    "questions_embs=[]\n",
    "for i,question_document in enumerate(questions):\n",
    "    temp_questions_embs=[]\n",
    "    for question in question_document:\n",
    "        temp_questions_embs.append(sentence_embedding(ft_model,[question]))\n",
    "    questions_embs.append(temp_questions_embs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,document in enumerate(documents_embs):\n",
    "\n",
    "    pairs_number=len(questions_embs[i])\n",
    "    pairs=[]\n",
    "    labels=[]\n",
    "    #  true pairs\n",
    "    for question in questions_embs[i]:\n",
    "        pairs.append([document,question])\n",
    "        labels.append(1)\n",
    "        \n",
    "    # false document\n",
    "    for j in range(pairs_number):\n",
    "        flag=True\n",
    "        rnd_indx=np.random.randint(0,len(documents_embs))\n",
    "        while  rnd_indx==i:\n",
    "                rnd_indx=np.random.randint(0,len(documents_embs))\n",
    "        pairs.append([documents_embs[rnd_indx],question])\n",
    "        labels.append(0)\n",
    "        \n",
    "    # false questions\n",
    "    for j in range(pairs_number):\n",
    "        flag=True\n",
    "        rnd_indx=np.random.randint(0,len(documents_embs))\n",
    "        while rnd_indx==i:\n",
    "                rnd_indx=np.random.randint(0,len(documents_embs))\n",
    "                \n",
    "        rnd_indx_question=np.random.randint(0,len(questions_embs[rnd_indx]))\n",
    "        pairs.append([document,questions_embs[rnd_indx][rnd_indx_question]])\n",
    "        labels.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=np.array(pairs).astype('float32')\n",
    "labels=np.array(labels).astype('float32')\n",
    "x_train,x_test,y_train,y_test=train_test_split(pairs,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss_with_margin(margin):\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "    \n",
    "        square_pred = K.square(y_pred)\n",
    "        margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "        return K.mean( (1 - y_true) * square_pred+y_true*margin_square)\n",
    "    \n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    #\n",
    "    #   it is also good idea to use VGG model, but it requires powerfull GPU\n",
    "    #   model = Sequential(VGG16(weights='imagenet', include_top=False, input_shape=image_shape).layers)\n",
    "    #\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Reshape((15,20,1),input_shape=emb_shape))\n",
    "#    model.add(Conv2D(32,(3,3)))\n",
    "#    model.add(BatchNormalization())\n",
    "        \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_shape=(300,)\n",
    "base_network = get_model()\n",
    "\n",
    "input_a = Input(shape=emb_shape)\n",
    "vect_output_a = base_network(input_a)\n",
    "\n",
    "input_b = Input(shape=emb_shape)\n",
    "vect_output_b = base_network(input_b)\n",
    "\n",
    "x = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([vect_output_a, vect_output_b])\n",
    "output= Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = Model([input_a, input_b], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = RMSprop(  learning_rate=0.015)\n",
    "#optim = Adam(  learning_rate=0.015)\n",
    "model.compile(loss=contrastive_loss_with_margin(margin=1), optimizer=optim)\n",
    "history = model.fit([x_train[:,0],x_train[:,1]], \n",
    "                    y_train, \n",
    "                    epochs=30, \n",
    "                    batch_size=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() > 0.5\n",
    "    return round(np.mean(pred == y_true),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = round(model.evaluate(x=[x_test[:,0],x_test[:,1]], y=y_test),4)\n",
    "\n",
    "y_pred_train = model.predict([x_train[:,0],x_train[:,1]])\n",
    "train_accuracy = compute_accuracy(y_train, y_pred_train)\n",
    "\n",
    "y_pred_test = model.predict([x_test[:,0], x_test[:,1]])\n",
    "test_accuracy = compute_accuracy(y_test, y_pred_test)\n",
    "\n",
    "print(\"Loss = {}, Train Accuracy = {} Test Accuracy = {}\".format(loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model, configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "[Reference](https://medium.com/swlh/fine-tuning-bert-for-text-classification-and-question-answering-using-tensorflow-framework-4d09daeb3330#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjFiZjhhODRkM2VjZDc3ZTlmMmFkNWYwNmZmZDI2MDcwMWRkMDZkOTAiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MjU5MDYxNDIsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwNjU4MDUxNjE5NzU3NTk0MTk2NCIsImhkIjoibWllbS5oc2UucnUiLCJlbWFpbCI6ImRna2FncmFtYW55YW5AbWllbS5oc2UucnUiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiYXpwIjoiMjE2Mjk2MDM1ODM0LWsxazZxZTA2MHMydHAyYTJqYW00bGpkY21zMDBzdHRnLmFwcHMuZ29vZ2xldXNlcmNvbnRlbnQuY29tIiwibmFtZSI6ItCU0LDQstC40LQg0JrQsNCz0YDQsNC80LDQvdGP0L0iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EtL0FPaDE0R2hGdEZGSkZMaDlyUFV0QXdZLXlyZWlRczJObjEyQ2xpc3hmX3hQPXM5Ni1jIiwiZ2l2ZW5fbmFtZSI6ItCU0LDQstC40LQiLCJmYW1pbHlfbmFtZSI6ItCa0LDQs9GA0LDQvNCw0L3Rj9C9IiwiaWF0IjoxNjI1OTA2NDQyLCJleHAiOjE2MjU5MTAwNDIsImp0aSI6IjhiYmVjYmRjYWRiZTAyZDZkNjQ1MTMxY2Q2NzJhZTI4MWViMTdkNWYifQ.LRr773uwpuGgpYnO7WsltRMVbC3JdFg7DBIlPtnLhN11OpKyNZ5X3y2ZGLx72_tJaSXZS52xdLcEsLuM4Tk1Ta_4ifnuAMkkeJMYJct0DpJOqXGYWS9S2atl8JLvbQaLyNrseNHneLtzRgtj--Htk1Lq0red-VmGxO849tzhKfjfQDPw-PsKxnhKMxHoEHJs91z-djL_L8ATQ6p86TcSWzWGsi4Ya69TmkMiRw-W2eMaZjN11gjSXUCnJKUUC122fluOmDzsGnUbtpoSGD-86mTa368FTFLbQSC7M8SGiZKo_pIZCcKrGjBQM-bJFAMs3-I8J7nM6782Vlh_CZiBew)\n",
    "\n",
    "[просто ссылка с хорошей инфой]( https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.contrib.skills.similarity_matching_skill import SimilarityMatchingSkill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_skill = SimilarityMatchingSkill(data_path = 'http://files.deeppavlov.ai/faq/school/faq_school.csv',\n",
    "                              x_col_name = 'Question', \n",
    "                              y_col_name = 'Answer',\n",
    "                              save_load_path = './model',\n",
    "                              config_type = 'tfidf_autofaq',\n",
    "                              train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/distilrubert-base-cased-conversational\")\n",
    "model = AutoModel.from_pretrained(\"DeepPavlov/distilrubert-base-cased-conversational\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "print(embed_bert_cls('привет мир', model, tokenizer).shape)\n",
    "# (312,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=tokenizer(['Хочу кредитную карту','Как взять кредит','Хочу взять кредит наличными'],padding=True)\n",
    "\n",
    "text=['Хочу кредитную карту','Хочу взять кредит наличными',\n",
    "      'Кредитование','Страхование','Инвестиции']\n",
    "tokens_vectors=get_embedding(ft_model,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks=np.array(tokens['attention_mask'])\n",
    "ids=np.array(tokens['input_ids'])\n",
    "data=[np.concatenate([ids,masks],axis=1)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = tokenizer(\"Хочу кредитную карту\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.pairwise.cosine_similarity(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummaryX import summary\n",
    "arch = summary(model, torch.rand((1, 3, 256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(context,questions,answer):\n",
    "    \n",
    "    t_context = tokenizer(context,padding=True)\n",
    "    t_questions = tokenizer(questions,padding=True)\n",
    "        \n",
    "    # create inputs as usual\n",
    "    input_ids = t_context['input_ids'] + t_questions['input_ids']\n",
    "    token_type_ids = [0] * len(t_context['input_ids']) + t_questions['attention_mask']\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    return input_ids,token_type_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=list(faq1[:,1].reshape((1,-1))[0])\n",
    "context=\" \".join(questions)\n",
    "input_ids,token_type_ids,attention_mask=preprocess(context,questions,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(context.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=questions[0].replace('\\n','')\n",
    "len(string.replace('/','').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "        self.start_token_idx = -1\n",
    "        self.end_token_idx = -1\n",
    "\n",
    "    def preprocess(self):\n",
    "        # clean context and question\n",
    "        context = \" \".join(str(self.context).split())\n",
    "        question = \" \".join(str(self.question).split())\n",
    "        # tokenize context and question\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "        \n",
    "        # if this is validation or training sample, preprocess answer\n",
    "        if self.answer_text is not None:\n",
    "            answer = \" \".join(str(self.answer_text).split())\n",
    "            # check if end character index is in the context\n",
    "            end_char_idx = self.start_char_idx + len(answer)\n",
    "            if end_char_idx >= len(context):\n",
    "                self.skip = True\n",
    "                return\n",
    "            # mark all the character indexes in context that are also in answer     \n",
    "            is_char_in_ans = [0] * len(context)\n",
    "            for idx in range(self.start_char_idx, end_char_idx):\n",
    "                is_char_in_ans[idx] = 1\n",
    "            ans_token_idx = []\n",
    "            # find all the tokens that are in the answers\n",
    "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "                if sum(is_char_in_ans[start:end]) > 0:\n",
    "                    ans_token_idx.append(idx)\n",
    "            if len(ans_token_idx) == 0:\n",
    "                self.skip = True\n",
    "                return\n",
    "            # get start and end token indexes\n",
    "            self.start_token_idx = ans_token_idx[0]\n",
    "            self.end_token_idx = ans_token_idx[-1]\n",
    "        # create inputs as usual\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        # add padding if necessary\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        self.input_word_ids = input_ids\n",
    "        self.input_type_ids = token_type_ids\n",
    "        self.input_mask = attention_mask\n",
    "        self.context_token_to_char = tokenized_context.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_squad_examples(raw_data):\n",
    "    squad_examples = []\n",
    "    for item in raw_data[\"data\"]:\n",
    "        for para in item[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                if \"answers\" in qa:\n",
    "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
    "                    start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
    "                    squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\n",
    "                else:\n",
    "                    squad_eg = Sample(question, context)\n",
    "                squad_eg.preprocess()\n",
    "                squad_examples.append(squad_eg)\n",
    "    return squad_examples\n",
    "\n",
    "\n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        \"input_word_ids\": [],\n",
    "        \"input_type_ids\": [],\n",
    "        \"input_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "        if item.skip == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "    x = [dataset_dict[\"input_word_ids\"],\n",
    "         dataset_dict[\"input_mask\"],\n",
    "         dataset_dict[\"input_type_ids\"]]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        # remove redundant whitespaces\n",
    "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
    "        # remove articles\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        text = re.sub(regex, \" \", text)\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # get the offsets of the first and last tokens of predicted answers\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        # for every pair of offsets\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            # take the required Sample object with the ground-truth answers in it\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            # use offsets to get back the span of text corresponding to\n",
    "            # our predicted first and last tokens\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "            normalized_pred_ans = self.normalize_text(pred_ans)\n",
    "            # clean the real answers\n",
    "            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            # check if the predicted answer is in an array of the ground-truth answers\n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
    "eval_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
    "\n",
    "with open(train_path) as f:\n",
    "    raw_train_data = json.load(f)\n",
    "    \n",
    "with open(eval_path) as f:\n",
    "    raw_eval_data = json.load(f)\n",
    "\n",
    "max_seq_length = 384\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
    "train_squad_examples = create_squad_examples(raw_train_data)\n",
    "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n",
    "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
    "start_logits = layers.Flatten()(start_logits)\n",
    "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
    "end_logits = layers.Flatten()(end_logits)\n",
    "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=8, callbacks=[ValidationCallback(x_eval, y_eval)])\n",
    "model.save_weights(\"./weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"data\":\n",
    "    [\n",
    "        {\"title\": \"Project Apollo\",\n",
    "         \"paragraphs\": [\n",
    "             {\n",
    "                 \"context\": \"The Apollo program, also known as Project Apollo, was the third United States human \"\n",
    "                            \"spaceflight program carried out by the National Aeronautics and Space Administration (\"\n",
    "                            \"NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First \"\n",
    "                            \"conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to \"\n",
    "                            \"follow the one-man Project Mercury which put the first Americans in space, Apollo was \"\n",
    "                            \"later dedicated to President John F. Kennedy's national goal of landing a man on the \"\n",
    "                            \"Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in \"\n",
    "                            \"a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project \"\n",
    "                            \"Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, \"\n",
    "                            \"and was supported by the two man Gemini program which ran concurrently with it from 1962 \"\n",
    "                            \"to 1966. Gemini missions developed some of the space travel techniques that were \"\n",
    "                            \"necessary for the success of the Apollo missions. Apollo used Saturn family rockets as \"\n",
    "                            \"launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications \"\n",
    "                            \"Program, which consisted of Skylab, a space station that supported three manned missions \"\n",
    "                            \"in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the \"\n",
    "                            \"Soviet Union in 1975.\",\n",
    "                 \"qas\": [\n",
    "                     {\"question\": \"What project put the first Americans into space?\",\n",
    "                      \"id\": \"Q1\"\n",
    "                      },\n",
    "                     {\"question\": \"What program was created to carry out these projects and missions?\",\n",
    "                      \"id\": \"Q2\"\n",
    "                      },\n",
    "                     {\"question\": \"What year did the first manned Apollo flight occur?\",\n",
    "                      \"id\": \"Q3\"\n",
    "                      },\n",
    "                     {\"question\": \"What President is credited with the original notion of putting Americans in space?\",\n",
    "                      \"id\": \"Q4\"\n",
    "                      },\n",
    "                     {\"question\": \"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\",\n",
    "                      \"id\": \"Q5\"\n",
    "                      },\n",
    "                     {\"question\": \"How long did Project Apollo run?\",\n",
    "                      \"id\": \"Q6\"\n",
    "                      },\n",
    "                     {\"question\": \"What program helped develop space travel techniques that Project Apollo used?\",\n",
    "                      \"id\": \"Q7\"\n",
    "                      },\n",
    "                     {\"question\": \"What space station supported three manned missions in 1973-1974?\",\n",
    "                      \"id\": \"Q8\"\n",
    "                      }\n",
    "                 ]}]}]}\n",
    "\n",
    "test_samples = create_squad_examples(data)\n",
    "x_test, _ = create_inputs_targets(test_samples)\n",
    "pred_start, pred_end = model.predict(x_test)\n",
    "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "    test_sample = test_samples[idx]\n",
    "    offsets = test_sample.context_token_to_char\n",
    "    start = np.argmax(start)\n",
    "    end = np.argmax(end)\n",
    "    pred_ans = None\n",
    "    if start >= len(offsets):\n",
    "        continue\n",
    "    pred_char_start = offsets[start][0]\n",
    "    if end < len(offsets):\n",
    "        pred_ans = test_sample.context[pred_char_start:offsets[end][1]]\n",
    "    else:\n",
    "        pred_ans = test_sample.context[pred_char_start:]\n",
    "    print(\"Q: \" + test_sample.question)\n",
    "    print(\"A: \" + pred_ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
